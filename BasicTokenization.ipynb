{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJVvZ/WU6t5v0r/RBrvnY0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sheshrajPawar/Activity_Detection/blob/main/BasicTokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TNhBopGuweXR",
        "outputId": "4aba5efd-0400-4e35-e53f-c4e1d87e0c22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'WORD'), ('how', 'WORD'), ('are', 'WORD'), ('you', 'WORD'), ('doing', 'WORD'), ('today', 'WORD'), ('123', 'WORD'), ('123', 'NUMBER'), (',', 'PUNCTUATION'), ('?', 'PUNCTUATION')]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Define regular expression patterns for tokenization\n",
        "    patterns = [\n",
        "        (r'\\b\\w+\\b', 'WORD'),      # Match word characters\n",
        "        (r'\\d+', 'NUMBER'),        # Match numbers\n",
        "        (r'[^\\w\\s]', 'PUNCTUATION') # Match punctuation\n",
        "    ]\n",
        "\n",
        "    tokens = []\n",
        "    for pattern, tag in patterns:\n",
        "        tokens.extend([(token, tag) for token in re.findall(pattern, text)])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example usage:\n",
        "text = \"Hello, how are you doing today? 123\"\n",
        "tokens = tokenize(text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize(text):\n",
        "    # Define regular expression patterns for tokenization\n",
        "    patterns = [\n",
        "        (r'\\b\\w+\\b', 'WORD'),         # Match word characters\n",
        "        (r'\\d+', 'NUMBER'),           # Match numbers\n",
        "        (r'[^\\w\\s]', 'PUNCTUATION'),  # Match punctuation\n",
        "        (r'\\s+', 'SPACE')             # Match whitespace\n",
        "    ]\n",
        "\n",
        "    tokens = []\n",
        "    for pattern, tag in patterns:\n",
        "        # Find all non-overlapping matches of the pattern in the input text\n",
        "        found_tokens = re.findall(pattern, text)\n",
        "        # Extend the tokens list with tuples containing the found tokens and their corresponding tag\n",
        "        tokens.extend([(token, tag) for token in found_tokens])\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Example usage:\n",
        "text = \"Hello, how are you doing today? 123\"\n",
        "tokens = tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_09ppw5Awuxm",
        "outputId": "543f37bf-172f-4fbe-afcf-0078c513767b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Hello', 'WORD'), ('how', 'WORD'), ('are', 'WORD'), ('you', 'WORD'), ('doing', 'WORD'), ('today', 'WORD'), ('123', 'WORD'), ('123', 'NUMBER'), (',', 'PUNCTUATION'), ('?', 'PUNCTUATION'), (' ', 'SPACE'), (' ', 'SPACE'), (' ', 'SPACE'), (' ', 'SPACE'), (' ', 'SPACE'), (' ', 'SPACE')]\n"
          ]
        }
      ]
    }
  ]
}